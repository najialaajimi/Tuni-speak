{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DUDKqmr4mZNd",
        "outputId": "0b163740-827a-434a-9749-7fdbe77418bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Initialisation du syst√®me TuniSpeak avec NLP et ML...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ NLTK resources downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Analyseur de sentiment charg√©\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Mod√®le NER charg√©\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1625226200.py:395: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
            "  faq_df.fillna(\"\", inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es FAQ charg√©es: 60 entr√©es\n",
            "üåç Langues disponibles: ['fr' 'ar' 'dr']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Mod√®le QA charg√© avec succ√®s\n",
            "‚úÖ Pr√©processeur AraBERT charg√©\n",
            "‚úÖ Embeddings cr√©√©s pour la recherche s√©mantique\n",
            "üöÄ Cr√©ation de l'interface Gradio am√©lior√©e...\n",
            "\n",
            "============================================================\n",
            "üß™ TESTS\n",
            "============================================================\n",
            "\n",
            "--- Test: Comment m'inscrire √† l'universit√© ? ---\n",
            "\n",
            "üîç Traitement de la question: Comment m'inscrire √† l'universit√© ?\n",
            "üåç Langue d√©tect√©e: fr\n",
            "üß† Analyse NLP effectu√©e\n",
            "‚úÖ FAQ similaire trouv√©e (Score: 1.000)\n",
            "R√©ponse: Pour vous inscrire, rendez-vous sur le portail d'inscription, cr√©ez un compte et soumettez les pi√®ce...\n",
            "Source: FAQ (Score: 1.000)\n",
            "Sentiment: 3 stars\n",
            "\n",
            "--- Test: ŸÉŸäŸÅ ÿ£ÿ≥ÿ¨ŸÑ ŸÅŸä ÿßŸÑÿ¨ÿßŸÖÿπÿ©ÿü ---\n",
            "\n",
            "üîç Traitement de la question: ŸÉŸäŸÅ ÿ£ÿ≥ÿ¨ŸÑ ŸÅŸä ÿßŸÑÿ¨ÿßŸÖÿπÿ©ÿü\n",
            "üåç Langue d√©tect√©e: ar\n",
            "üß† Analyse NLP effectu√©e\n",
            "‚úÖ FAQ similaire trouv√©e (Score: 1.000)\n",
            "R√©ponse: ŸÑŸÑÿ™ÿ≥ÿ¨ŸäŸÑÿå ÿßÿØÿÆŸÑ ÿ•ŸÑŸâ ÿ®Ÿàÿßÿ®ÿ© ÿßŸÑÿ™ÿ≥ÿ¨ŸäŸÑÿå ÿ£ŸÜÿ¥ÿ¶ ÿ≠ÿ≥ÿßÿ®Ÿãÿß Ÿàÿßÿ±ŸÅÿπ ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ© (ÿ®ÿ∑ÿßŸÇÿ© ŸáŸàŸäÿ©ÿå ŸÉÿ¥ŸÅ ÿØÿ±ÿ¨ÿßÿ™)....\n",
            "Source: FAQ (Score: 1.000)\n",
            "Sentiment: 5 stars\n",
            "\n",
            "--- Test: Quels sont les frais de scolarit√© ? ---\n",
            "\n",
            "üîç Traitement de la question: Quels sont les frais de scolarit√© ?\n",
            "üåç Langue d√©tect√©e: fr\n",
            "üß† Analyse NLP effectu√©e\n",
            "‚úÖ FAQ similaire trouv√©e (Score: 0.920)\n",
            "R√©ponse: Les frais varient selon le programme. Consultez la page 'Frais' du site de l'universit√© ou le servic...\n",
            "Source: FAQ (Score: 0.920)\n",
            "Sentiment: 3 stars\n",
            "\n",
            "============================================================\n",
            "‚úÖ SYST√àME PR√äT avec NLP et ML !\n",
            "============================================================\n",
            "üìä FAQ: 60 | Interactions: 3\n",
            "üöÄ Lancement de Gradio...\n",
            "* Running on public URL: https://d97e9ef1f85b530227.gradio.live\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d97e9ef1f85b530227.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Lancement FastAPI...\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/index-CCA5X64I.css HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/index-DputZZxm.js HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [139]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåç Application: https://pseudocartilaginous-secantly-helen.ngrok-free.dev\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/svelte/svelte.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Index-CDBfCghh.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/init-CYI6iZYC.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/init-DV_66Kck.css HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/MarkdownCode-Ca9FjBJK.css HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/FullscreenButton-BYduS5IX.css HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/StreamingBar-Dq7cCzkm.css HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Index-BkyjjaNE.css HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/StreamingBar-BtwDBaPl.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/FullscreenButton-D7DmzQQ9.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/MarkdownCode-DTX-UkhB.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/prism-python-oCRsS_S4.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/index-CWx9LkNn.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /theme.css?v=2fa39f6df94156f7ef9d84370dbb2657d672827c75f082ac995879c32290c5ca HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Blocks-BEaKnzuj.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Blocks-D0rbzfnq.css HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Index-D2wzIuuw.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Example-2_XpXLIx.css HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Index-CRfssLQd.css HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Index-Ci4eVLjY.css HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Index-Bp6-wsMo.css HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Example-DRnq2kzJ.css HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Index-D812IjdW.css HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/ModifyUpload-REHVxGq-.css HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Example-BjJvNKVN.css HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Index-C-vZ3ifd.css HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Index-a-pQRFMj.css HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Index-DMXFGchm.css HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/ImagePreview-D5bl4ZzZ.css HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Image-Ds1uFAjO.css HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Index-VLJFaMM1.css HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Index-mr0iIpbh.css HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Example-CTIV4tuG.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Index-CzTqaf_5.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Index-DcuC0IYx.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Index-CHBVi5hu.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Example-Bdcw8sPw.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Index-CIdRvN_d.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/ModifyUpload-CMsAygJb.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Example-cR3xzyOo.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Index-D80jikzp.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Index-cwl6r4yK.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Index-Dwn46RCV.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Image-0bVIZe-w.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/ImagePreview-BiSVoKtP.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Index-pH7dzLsN.js HTTP/1.1\" 200 OK\n",
            "INFO:     172.31.14.164:0 - \"GET /assets/Example-D5YxIsQj.js HTTP/1.1\" 200 OK\n",
            "INFO:     197.14.218.132:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "\n",
            "üîç Traitement de la question: Comment m'inscrire √† l'universit√©?\n",
            "üåç Langue d√©tect√©e: fr\n",
            "üß† Analyse NLP effectu√©e\n",
            "‚úÖ FAQ similaire trouv√©e (Score: 1.000)\n",
            "INFO:     197.14.218.132:0 - \"POST /qa HTTP/1.1\" 200 OK\n",
            "INFO:     197.14.218.132:0 - \"GET /learning-report HTTP/1.1\" 200 OK\n",
            "\n",
            "üîç Traitement de la question: Quelle est la capitale de la France?\n",
            "üåç Langue d√©tect√©e: fr\n",
            "üß† Analyse NLP effectu√©e\n",
            "‚ö†Ô∏è Similarit√© FAQ insuffisante (0.333)\n",
            "ü§ñ Utilisation de l'IA g√©n√©rative...\n",
            "INFO:     197.14.218.132:0 - \"POST /qa HTTP/1.1\" 200 OK\n",
            "INFO:     197.14.218.132:0 - \"GET /learning-report HTTP/1.1\" 200 OK\n",
            "INFO:     197.14.218.132:0 - \"GET / HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [139]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1625226200.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m \u001b[0mlaunch_fastapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1625226200.py\u001b[0m in \u001b[0;36mlaunch_fastapi\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_destroy_pending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36m_run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                     \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0;31m# restore the current task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/events.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSystemExit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/tasks.py\u001b[0m in \u001b[0;36m__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;31m# instead of `__next__()`, which is slower for futures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;31m# that return non-generator iterators from their `__iter__`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Needed to break cycles when an exception occurs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(self, exc)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0m_enter_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__step_run_and_handle_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0m_leave_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\u001b[0m in \u001b[0;36mserve\u001b[0;34m(self, sockets)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msockets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_signals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_serve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msockets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\u001b[0m in \u001b[0;36mcapture_signals\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# done LIFO, see https://stackoverflow.com/questions/48434964\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcaptured_signal\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_captured_signals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptured_signal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhandle_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFrameType\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "!pip install xai-sdk --quiet\n",
        "!pip install transformers torch sentencepiece langdetect PyPDF2 scikit-learn gradio fastapi uvicorn nest-asyncio arabic-reshaper python-bidi pyngrok arabert==1.0.1 nltk google-generativeai python-docx spacy textblob --quiet\n",
        "\n",
        "import nest_asyncio\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from transformers import pipeline\n",
        "import langdetect\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "import gradio as gr\n",
        "from fastapi import FastAPI, Request, Form, File, UploadFile\n",
        "from fastapi.responses import HTMLResponse\n",
        "from fastapi.templating import Jinja2Templates\n",
        "import uvicorn\n",
        "import threading\n",
        "import asyncio\n",
        "import requests\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "import nltk\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import google.generativeai as genai\n",
        "from google.generativeai import types\n",
        "import PyPDF2\n",
        "from docx import Document\n",
        "import tempfile\n",
        "import json\n",
        "from datetime import datetime\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# Configuration initiale\n",
        "print(\"üîß Initialisation du syst√®me TuniSpeak avec NLP et ML...\")\n",
        "\n",
        "# Fetch GEMINI_API_KEY\n",
        "GEMINI_API_KEY = None\n",
        "try:\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    if not GEMINI_API_KEY:\n",
        "        print(\"‚ö†Ô∏è Warning: GEMINI_API_KEY secret not found. Generative AI fallback will not work.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error fetching GEMINI_API_KEY secret: {e}\")\n",
        "\n",
        "# Download NLTK resources\n",
        "try:\n",
        "    nltk.download('punkt_tab')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    nltk.download('wordnet')\n",
        "    print(\"‚úÖ NLTK resources downloaded\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Erreur lors du t√©l√©chargement des ressources NLTK : {e}\")\n",
        "\n",
        "# Patch pour ex√©cuter uvicorn dans Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# =============================================================================\n",
        "# MODULE D'APPRENTISSAGE - Collecte et analyse des interactions\n",
        "# =============================================================================\n",
        "\n",
        "class LearningModule:\n",
        "    \"\"\"Module d'apprentissage qui am√©liore le syst√®me au fil du temps\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.interactions_log = []\n",
        "        self.feedback_log = []\n",
        "        self.intent_classifier = None\n",
        "        self.user_patterns = defaultdict(list)\n",
        "        self.learning_data_file = \"/content/learning_data.json\"\n",
        "        self.load_learning_data()\n",
        "\n",
        "    def _get_language_name(self, code):\n",
        "        \"\"\"Convertit le code langue en nom complet\"\"\"\n",
        "        language_map = {\n",
        "            'fr': 'Fran√ßais',\n",
        "            'ar': 'Arabe',\n",
        "            'dar': 'Darija Tunisienne'\n",
        "        }\n",
        "        return language_map.get(code, code)\n",
        "\n",
        "    def load_learning_data(self):\n",
        "        \"\"\"Charge les donn√©es d'apprentissage pr√©c√©dentes\"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.learning_data_file):\n",
        "                with open(self.learning_data_file, 'r', encoding='utf-8') as f:\n",
        "                    data = json.load(f)\n",
        "                    self.interactions_log = data.get('interactions', [])\n",
        "                    self.feedback_log = data.get('feedback', [])\n",
        "                print(f\"‚úÖ Donn√©es d'apprentissage charg√©es: {len(self.interactions_log)} interactions\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Impossible de charger les donn√©es d'apprentissage: {e}\")\n",
        "\n",
        "    def save_learning_data(self):\n",
        "        \"\"\"Sauvegarde les donn√©es d'apprentissage\"\"\"\n",
        "        try:\n",
        "            data = {\n",
        "                'interactions': self.interactions_log[-1000:],  # Garder les 1000 derni√®res\n",
        "                'feedback': self.feedback_log[-500:]  # Garder les 500 derniers\n",
        "            }\n",
        "            with open(self.learning_data_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur sauvegarde apprentissage: {e}\")\n",
        "\n",
        "    def log_interaction(self, question, answer, source, language, similarity_score=None):\n",
        "        \"\"\"Enregistre une interaction pour l'apprentissage\"\"\"\n",
        "        interaction = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'question': question,\n",
        "            'answer': answer[:200],  # Tronquer pour √©conomiser l'espace\n",
        "            'source': source,\n",
        "            'language': language,\n",
        "            'similarity_score': similarity_score\n",
        "        }\n",
        "        self.interactions_log.append(interaction)\n",
        "\n",
        "        # Sauvegarder tous les 10 interactions\n",
        "        if len(self.interactions_log) % 10 == 0:\n",
        "            self.save_learning_data()\n",
        "\n",
        "    def log_feedback(self, question, was_helpful, comment=None):\n",
        "        \"\"\"Enregistre le feedback utilisateur\"\"\"\n",
        "        feedback = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'question': question,\n",
        "            'was_helpful': was_helpful,\n",
        "            'comment': comment\n",
        "        }\n",
        "        self.feedback_log.append(feedback)\n",
        "        self.save_learning_data()\n",
        "\n",
        "    def analyze_patterns(self):\n",
        "        \"\"\"Analyse les patterns d'utilisation\"\"\"\n",
        "        if len(self.interactions_log) == 0:\n",
        "            return \"üìä **Rapport d'Apprentissage**\\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\\nAucune donn√©e disponible. Posez quelques questions pour g√©n√©rer un rapport !\"\n",
        "\n",
        "        # Analyse toujours, m√™me avec peu de donn√©es\n",
        "        languages = [i['language'] for i in self.interactions_log]\n",
        "        lang_counts = Counter(languages)\n",
        "\n",
        "        sources = [i['source'] for i in self.interactions_log]\n",
        "        source_counts = Counter(sources)\n",
        "\n",
        "        # Calculer les scores de similarit√© (uniquement pour FAQ)\n",
        "        faq_scores = [i['similarity_score'] for i in self.interactions_log\n",
        "                      if i['source'] == 'FAQ' and i['similarity_score'] is not None]\n",
        "        avg_faq_score = np.mean(faq_scores) if faq_scores else 0\n",
        "\n",
        "        # Calculer les statistiques de feedback\n",
        "        helpful_feedback = [f for f in self.feedback_log if f['was_helpful']]\n",
        "        helpful_rate = len(helpful_feedback) / len(self.feedback_log) * 100 if self.feedback_log else 0\n",
        "\n",
        "        # Cr√©er le rapport\n",
        "        report_lines = [\n",
        "            \"üìä **Rapport d'Apprentissage**\",\n",
        "            \"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\",\n",
        "            f\"üìà Total interactions: {len(self.interactions_log)}\",\n",
        "            f\"üí¨ Feedbacks re√ßus: {len(self.feedback_log)}\",\n",
        "            f\"üëç Taux de satisfaction: {helpful_rate:.1f}%\",\n",
        "            \"\",\n",
        "            \"üåç **Distribution des langues:**\"\n",
        "        ]\n",
        "\n",
        "        # Ajouter les langues\n",
        "        for lang, count in lang_counts.most_common():\n",
        "            percentage = count / len(languages) * 100\n",
        "            report_lines.append(f\"  ‚Ä¢ {self._get_language_name(lang)}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "        report_lines.append(\"\")\n",
        "        report_lines.append(\"üìö **Sources utilis√©es:**\")\n",
        "\n",
        "       # Ajouter les sources\n",
        "        for source, count in source_counts.most_common():\n",
        "            percentage = count / len(sources) * 100\n",
        "            report_lines.append(f\"  ‚Ä¢ {source}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "       # Ajouter les performances si pertinent\n",
        "        if faq_scores:\n",
        "           report_lines.append(\"\")\n",
        "           report_lines.append(\"üéØ **Performance FAQ:**\")\n",
        "           report_lines.append(f\"  ‚Ä¢ Score de similarit√© moyen: {avg_faq_score:.3f}\")\n",
        "           report_lines.append(f\"  ‚Ä¢ Nombre de r√©ponses FAQ: {len(faq_scores)}\")\n",
        "\n",
        "    # Ajouter des conseils si peu de donn√©es\n",
        "        if len(self.interactions_log) < 10:\n",
        "          report_lines.append(\"\")\n",
        "          report_lines.append(\"üí° **Conseil:** Continuez √† poser des questions pour obtenir une analyse plus pr√©cise!\")\n",
        "\n",
        "        return \"\\n\".join(report_lines)\n",
        "\n",
        "    def train_intent_classifier(self, questions, intents):\n",
        "        \"\"\"Entra√Æne un classificateur d'intentions\"\"\"\n",
        "        try:\n",
        "            from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "            from sklearn.naive_bayes import MultinomialNB\n",
        "            from sklearn.pipeline import Pipeline\n",
        "\n",
        "            # Cr√©er un pipeline\n",
        "            self.intent_classifier = Pipeline([\n",
        "                ('tfidf', TfidfVectorizer(max_features=1000)),\n",
        "                ('clf', MultinomialNB())\n",
        "            ])\n",
        "\n",
        "            # Entra√Æner\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                questions, intents, test_size=0.2, random_state=42\n",
        "            )\n",
        "\n",
        "            self.intent_classifier.fit(X_train, y_train)\n",
        "\n",
        "            # √âvaluer\n",
        "            y_pred = self.intent_classifier.predict(X_test)\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "            print(f\"‚úÖ Classificateur d'intentions entra√Æn√© (Accuracy: {accuracy:.3f})\")\n",
        "            return accuracy\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur entra√Ænement classificateur: {e}\")\n",
        "            return 0\n",
        "\n",
        "    def predict_intent(self, question):\n",
        "        \"\"\"Pr√©dit l'intention d'une question\"\"\"\n",
        "        if self.intent_classifier:\n",
        "            try:\n",
        "                return self.intent_classifier.predict([question])[0]\n",
        "            except:\n",
        "                return \"unknown\"\n",
        "        return \"unknown\"\n",
        "\n",
        "# Initialiser le module d'apprentissage\n",
        "learning_module = LearningModule()\n",
        "\n",
        "# =============================================================================\n",
        "# MODULE NLP AVANC√â - Analyse linguistique approfondie\n",
        "# =============================================================================\n",
        "\n",
        "class NLPModule:\n",
        "    \"\"\"Module NLP pour analyse linguistique avanc√©e\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.sentiment_analyzer = None\n",
        "        self.ner_model = None\n",
        "        self.init_models()\n",
        "\n",
        "    def init_models(self):\n",
        "        \"\"\"Initialise les mod√®les NLP\"\"\"\n",
        "        try:\n",
        "            # Analyseur de sentiment multilingue\n",
        "            self.sentiment_analyzer = pipeline(\n",
        "                \"sentiment-analysis\",\n",
        "                model=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "            )\n",
        "            print(\"‚úÖ Analyseur de sentiment charg√©\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Sentiment analyzer non disponible: {e}\")\n",
        "\n",
        "        try:\n",
        "            # NER (Named Entity Recognition)\n",
        "            self.ner_model = pipeline(\n",
        "                \"ner\",\n",
        "                model=\"Davlan/distilbert-base-multilingual-cased-ner-hrl\"\n",
        "            )\n",
        "            print(\"‚úÖ Mod√®le NER charg√©\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è NER model non disponible: {e}\")\n",
        "\n",
        "    def analyze_sentiment(self, text):\n",
        "        \"\"\"Analyse le sentiment du texte\"\"\"\n",
        "        if not self.sentiment_analyzer:\n",
        "            return {\"label\": \"NEUTRAL\", \"score\": 0.5}\n",
        "\n",
        "        try:\n",
        "            result = self.sentiment_analyzer(text[:512])[0]\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur analyse sentiment: {e}\")\n",
        "            return {\"label\": \"NEUTRAL\", \"score\": 0.5}\n",
        "\n",
        "    def extract_entities(self, text):\n",
        "        \"\"\"Extrait les entit√©s nomm√©es\"\"\"\n",
        "        if not self.ner_model:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            entities = self.ner_model(text[:512])\n",
        "            # Grouper les entit√©s\n",
        "            grouped = []\n",
        "            current_entity = None\n",
        "\n",
        "            for ent in entities:\n",
        "                if ent['entity'].startswith('B-'):\n",
        "                    if current_entity:\n",
        "                        grouped.append(current_entity)\n",
        "                    current_entity = {\n",
        "                        'text': ent['word'],\n",
        "                        'type': ent['entity'][2:],\n",
        "                        'score': ent['score']\n",
        "                    }\n",
        "                elif ent['entity'].startswith('I-') and current_entity:\n",
        "                    current_entity['text'] += ' ' + ent['word']\n",
        "                    current_entity['score'] = (current_entity['score'] + ent['score']) / 2\n",
        "\n",
        "            if current_entity:\n",
        "                grouped.append(current_entity)\n",
        "\n",
        "            return grouped\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur extraction entit√©s: {e}\")\n",
        "            return []\n",
        "\n",
        "    def extract_keywords(self, text, top_n=5):\n",
        "        \"\"\"Extrait les mots-cl√©s importants\"\"\"\n",
        "        try:\n",
        "            from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "            # Tokeniser les phrases\n",
        "            sentences = nltk.sent_tokenize(text)\n",
        "            if len(sentences) < 2:\n",
        "                return []\n",
        "\n",
        "            # TF-IDF\n",
        "            vectorizer = TfidfVectorizer(max_features=top_n, stop_words='english')\n",
        "            tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "            # Obtenir les mots-cl√©s\n",
        "            feature_names = vectorizer.get_feature_names_out()\n",
        "            scores = tfidf_matrix.sum(axis=0).A1\n",
        "\n",
        "            keywords = [(feature_names[i], scores[i]) for i in range(len(feature_names))]\n",
        "            keywords.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            return [k[0] for k in keywords[:top_n]]\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur extraction keywords: {e}\")\n",
        "            return []\n",
        "\n",
        "    def analyze_text_complexity(self, text):\n",
        "        \"\"\"Analyse la complexit√© du texte\"\"\"\n",
        "        try:\n",
        "            words = text.split()\n",
        "            sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "            avg_word_length = np.mean([len(word) for word in words])\n",
        "            avg_sentence_length = len(words) / len(sentences) if sentences else 0\n",
        "\n",
        "            # Score de complexit√© simple\n",
        "            complexity_score = (avg_word_length * 0.5 + avg_sentence_length * 0.1) / 2\n",
        "\n",
        "            if complexity_score < 3:\n",
        "                level = \"Simple\"\n",
        "            elif complexity_score < 5:\n",
        "                level = \"Moyen\"\n",
        "            else:\n",
        "                level = \"Complexe\"\n",
        "\n",
        "            return {\n",
        "                'level': level,\n",
        "                'avg_word_length': round(avg_word_length, 2),\n",
        "                'avg_sentence_length': round(avg_sentence_length, 2),\n",
        "                'complexity_score': round(complexity_score, 2)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur analyse complexit√©: {e}\")\n",
        "            return {'level': 'Unknown', 'complexity_score': 0}\n",
        "\n",
        "    def full_analysis(self, text):\n",
        "        \"\"\"Analyse NLP compl√®te\"\"\"\n",
        "        analysis = {\n",
        "            'sentiment': self.analyze_sentiment(text),\n",
        "            'entities': self.extract_entities(text),\n",
        "            'keywords': self.extract_keywords(text),\n",
        "            'complexity': self.analyze_text_complexity(text)\n",
        "        }\n",
        "        return analysis\n",
        "\n",
        "# Initialiser le module NLP\n",
        "nlp_module = NLPModule()\n",
        "\n",
        "# =============================================================================\n",
        "# CHARGEMENT DES DONN√âES FAQ\n",
        "# =============================================================================\n",
        "\n",
        "def load_faq_data():\n",
        "    \"\"\"Charge les donn√©es FAQ depuis le fichier CSV fourni\"\"\"\n",
        "    try:\n",
        "        faq_df = pd.read_csv('tuni_speak_faq_sample.csv')\n",
        "        faq_df.fillna(\"\", inplace=True)\n",
        "        faq_df['text'] = faq_df.apply(\n",
        "            lambda row: f\"Question: {row['question']} R√©ponse: {row['answer']}\",\n",
        "            axis=1\n",
        "        )\n",
        "        print(f\"‚úÖ Donn√©es FAQ charg√©es: {len(faq_df)} entr√©es\")\n",
        "        print(f\"üåç Langues disponibles: {faq_df['language'].unique()}\")\n",
        "        return faq_df\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur lors du chargement des donn√©es FAQ: {e}\")\n",
        "        return pd.DataFrame(columns=['id', 'language', 'question', 'answer', 'text'])\n",
        "\n",
        "faq_df = load_faq_data()\n",
        "\n",
        "# =============================================================================\n",
        "# INITIALISATION DES MOD√àLES\n",
        "# =============================================================================\n",
        "\n",
        "model_name = \"deepset/xlm-roberta-large-squad2\"\n",
        "try:\n",
        "    qa_pipeline = pipeline(\"question-answering\", model=model_name, tokenizer=model_name)\n",
        "    print(\"‚úÖ Mod√®le QA charg√© avec succ√®s\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur lors du chargement du mod√®le QA : {e}\")\n",
        "    qa_pipeline = None\n",
        "\n",
        "try:\n",
        "    arabert_prep = ArabertPreprocessor(model_name=\"aubmindlab/bert-base-arabertv02\")\n",
        "    print(\"‚úÖ Pr√©processeur AraBERT charg√©\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Arabert non disponible, utilisant normalisation basique. Erreur : {e}\")\n",
        "    def arabert_prep_fallback(text):\n",
        "        return text.lower().strip()\n",
        "    arabert_prep = arabert_prep_fallback\n",
        "\n",
        "try:\n",
        "    embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "    if not faq_df.empty:\n",
        "        faq_df['normalized_question'] = faq_df.apply(\n",
        "            lambda row: normalize_text(row['question'], row['language']),\n",
        "            axis=1\n",
        "        )\n",
        "        faq_df['question_embeddings'] = faq_df['normalized_question'].apply(\n",
        "            lambda x: embedding_model.encode(x) if x and x != \"vide\" else embedding_model.encode(\"\")\n",
        "        )\n",
        "        print(\"‚úÖ Embeddings cr√©√©s pour la recherche s√©mantique\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Aucune donn√©e FAQ pour cr√©er les embeddings\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur lors de la cr√©ation des embeddings: {e}\")\n",
        "    embedding_model = None\n",
        "\n",
        "# =============================================================================\n",
        "# FONCTIONS DE BASE\n",
        "# =============================================================================\n",
        "\n",
        "def detect_language(text):\n",
        "    \"\"\"D√©tecte le langage avec meilleure d√©tection du darija\"\"\"\n",
        "    try:\n",
        "        if not text or not isinstance(text, str):\n",
        "            return \"fr\"\n",
        "\n",
        "        arabic_chars = re.compile(r'[\\u0600-\\u06FF]')\n",
        "        darija_patterns = [\n",
        "            r'\\b(ch|sh|9|3|7|khouya|lazma|chnouwa|kifech|n3mel|bach|ntsajel|m3ak|fama|wallah|yasser)\\b',\n",
        "            r'\\b(ya3tik|saha|yais|mouch|mch|ken|walah|hata|waktah)\\b',\n",
        "            r'\\b(bara|tawa|famma|kima|qadda|mta3|bech|t7eb)\\b'\n",
        "        ]\n",
        "\n",
        "        if arabic_chars.search(text):\n",
        "            text_lower = text.lower()\n",
        "            for pattern in darija_patterns:\n",
        "                if re.search(pattern, text_lower):\n",
        "                    return \"dar\"\n",
        "            return \"ar\"\n",
        "\n",
        "        return langdetect.detect(text) if text and text.strip() else \"fr\"\n",
        "    except Exception:\n",
        "        return \"fr\"\n",
        "\n",
        "def normalize_text(text, lang):\n",
        "    \"\"\"Nettoie et normalise le texte selon la langue\"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return \"vide\"\n",
        "    text = text.replace(\"\\n\", \" \").strip()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    if lang in [\"fr\", \"dar\"]:\n",
        "        text = text.lower()\n",
        "    if lang in [\"ar\", \"dar\"] and callable(getattr(arabert_prep, 'preprocess', None)):\n",
        "        text = arabert_prep.preprocess(text)\n",
        "    return text if text else \"vide\"\n",
        "\n",
        "# =============================================================================\n",
        "# FONCTIONS DE RECHERCHE ET R√âPONSE AM√âLIOR√âES\n",
        "# =============================================================================\n",
        "\n",
        "def find_similar_faq(query, threshold=0.6):\n",
        "    \"\"\"Recherche les FAQ similaires\"\"\"\n",
        "    if not query or not isinstance(query, str) or faq_df.empty or embedding_model is None:\n",
        "        return None, None, None, 0.0\n",
        "\n",
        "    lang = detect_language(query)\n",
        "    normalized_query = normalize_text(query, lang)\n",
        "\n",
        "    try:\n",
        "        query_embedding = embedding_model.encode(normalized_query)\n",
        "        query_embedding = query_embedding.reshape(1, -1)\n",
        "\n",
        "        faq_embeddings_list = [np.array(emb) for emb in faq_df['question_embeddings'].tolist()]\n",
        "        if not faq_embeddings_list:\n",
        "            return None, None, None, 0.0\n",
        "\n",
        "        faq_embeddings_matrix = np.vstack(faq_embeddings_list)\n",
        "        similarities = cosine_similarity(query_embedding, faq_embeddings_matrix).flatten()\n",
        "        most_similar_index = np.argmax(similarities)\n",
        "        similarity_score = similarities[most_similar_index]\n",
        "\n",
        "        if similarity_score >= threshold:\n",
        "            most_similar_faq = faq_df.iloc[most_similar_index]\n",
        "            return (most_similar_faq['question'],\n",
        "                    most_similar_faq['answer'],\n",
        "                    most_similar_faq['language'],\n",
        "                    float(similarity_score))\n",
        "        else:\n",
        "            return None, None, None, similarity_score\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error in find_similar_faq: {e}\")\n",
        "        return None, None, None, 0.0\n",
        "\n",
        "def configure_gemini_api(api_key):\n",
        "    \"\"\"Configure la cl√© API Gemini\"\"\"\n",
        "    global GEMINI_API_KEY\n",
        "    GEMINI_API_KEY = api_key.strip()\n",
        "    if GEMINI_API_KEY:\n",
        "        try:\n",
        "            genai.configure(api_key=GEMINI_API_KEY)\n",
        "            # Tester la connexion\n",
        "            test_model = genai.GenerativeModel('gemini-flash-latest')\n",
        "            test_response = test_model.generate_content(\"Test\")\n",
        "            return f\"‚úÖ Cl√© API configur√©e avec succ√®s ! Mod√®le test√©: {test_model.model_name}\"\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Erreur de configuration: {str(e)}\"\n",
        "    else:\n",
        "        return \"‚ö†Ô∏è Veuillez entrer une cl√© API valide\"\n",
        "\n",
        "def call_generative_ai(query):\n",
        "    \"\"\"IA g√©n√©rative qui r√©pond syst√©matiquement en 3 langues\"\"\"\n",
        "    if not GEMINI_API_KEY:\n",
        "        return \"üáπüá≥ ÿßŸÑÿØÿßÿ±ÿ¨ÿ© ÿßŸÑÿ™ŸàŸÜÿ≥Ÿäÿ©: ÿπÿ∞ÿ±Ÿãÿßÿå ŸÖŸÅŸäÿ¥ ÿ•ÿ¨ÿßÿ®ÿ© ÿ≠ÿßÿ∂ÿ±ÿ©.\\nüá∏üá¶ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑŸÅÿµÿ≠Ÿâ: ÿπÿ∞ÿ±Ÿãÿßÿå ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿ•ÿ¨ÿßÿ®ÿ© ŸÖÿ™ÿßÿ≠ÿ© ÿ≠ÿßŸÑŸäŸãÿß.\\nüá´üá∑ Fran√ßais: D√©sol√©, aucune r√©ponse disponible pour le moment. (Cl√© API non configur√©e)\", \"IA G√©n√©rative (Cl√© API manquante)\"\n",
        "\n",
        "    try:\n",
        "        genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "        system_prompt = \"\"\"Tu es un assistant trilingue sp√©cialis√©.\n",
        "        Pour CHAQUE question, tu DOIS r√©pondre en 3 langues dans cet ordre :\n",
        "        1. Darija tunisienne (ÿßŸÑŸÑŸáÿ¨ÿ© ÿßŸÑÿ™ŸàŸÜÿ≥Ÿäÿ©)\n",
        "        2. Arabe standard (ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑŸÅÿµÿ≠Ÿâ)\n",
        "        3. Fran√ßais\n",
        "\n",
        "        Format de r√©ponse OBLIGATOIRE :\n",
        "        üáπüá≥ ÿßŸÑÿØÿßÿ±ÿ¨ÿ© ÿßŸÑÿ™ŸàŸÜÿ≥Ÿäÿ©: [r√©ponse en darija]\n",
        "        üá∏üá¶ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑŸÅÿµÿ≠Ÿâ: [r√©ponse en arabre standard]\n",
        "        üá´üá∑ Fran√ßais: [r√©ponse en fran√ßais]\n",
        "\n",
        "        Sois concis et pr√©cis dans tes r√©ponses. Si tu ne connais pas la r√©ponse, dis-le clairement.\"\"\"\n",
        "\n",
        "        gemini_model = genai.GenerativeModel(\n",
        "            model_name='gemini-flash-latest',\n",
        "            system_instruction=system_prompt\n",
        "        )\n",
        "\n",
        "        # Configuration de s√©curit√©\n",
        "        generation_config = {\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.95,\n",
        "            \"top_k\": 40,\n",
        "            \"max_output_tokens\": 1024,\n",
        "        }\n",
        "\n",
        "        response = gemini_model.generate_content(\n",
        "            query,\n",
        "            generation_config=generation_config,\n",
        "            safety_settings={\n",
        "                types.HarmCategory.HARM_CATEGORY_HARASSMENT: types.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "                types.HarmCategory.HARM_CATEGORY_HATE_SPEECH: types.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        if response.text:\n",
        "            return response.text.strip()\n",
        "        else:\n",
        "            return \"üáπüá≥ ÿßŸÑÿØÿßÿ±ÿ¨ÿ© ÿßŸÑÿ™ŸàŸÜÿ≥Ÿäÿ©: ŸÖŸÅŸäÿ¥ ÿ•ÿ¨ÿßÿ®ÿ© ŸÖŸÜ ÿßŸÑÿ≥Ÿäÿ±⁄§ÿ±.\\nüá∏üá¶ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑŸÅÿµÿ≠Ÿâ: ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿ•ÿ¨ÿßÿ®ÿ© ŸÖŸÜ ÿßŸÑÿÆÿßÿØŸÖ.\\nüá´üá∑ Fran√ßais: Pas de r√©ponse du serveur.\", \"IA G√©n√©rative (Erreur)\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur d√©taill√©e lors de l'appel √† l'IA g√©n√©rative: {e}\")\n",
        "        return f\"\"\"üáπüá≥ ÿßŸÑÿØÿßÿ±ÿ¨ÿ© ÿßŸÑÿ™ŸàŸÜÿ≥Ÿäÿ©: ÿπÿ∞ÿ±Ÿãÿßÿå ÿ¨ÿßÿ±Ÿä ÿßŸÑÿµŸäÿßŸÜÿ© ÿßŸÑÿ™ŸÇŸÜŸäÿ©.\n",
        "üá∏üá¶ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑŸÅÿµÿ≠Ÿâ: ÿπÿ∞ÿ±Ÿãÿßÿå ÿßŸÑÿÆÿØŸÖÿ© ŸÇŸäÿØ ÿßŸÑÿµŸäÿßŸÜÿ© ÿßŸÑÿ™ŸÇŸÜŸäÿ©.\n",
        "üá´üá∑ Fran√ßais: D√©sol√©, le service est en maintenance technique.\n",
        "\n",
        "Erreur: {str(e)[:100]}\"\"\", \"IA G√©n√©rative (Erreur)\"\n",
        "\n",
        "def answer_question(query, include_nlp_analysis=False):\n",
        "    \"\"\"Fonction principale pour r√©pondre aux questions avec analyse NLP optionnelle\"\"\"\n",
        "    if not query or not isinstance(query, str):\n",
        "        return \"Veuillez entrer une question valide.\", \"N/A\", None\n",
        "\n",
        "    print(f\"\\nüîç Traitement de la question: {query}\")\n",
        "    lang = detect_language(query)\n",
        "    print(f\"üåç Langue d√©tect√©e: {lang}\")\n",
        "\n",
        "    # Analyse NLP si demand√©e\n",
        "    nlp_analysis = None\n",
        "    if include_nlp_analysis:\n",
        "        nlp_analysis = nlp_module.full_analysis(query)\n",
        "        print(f\"üß† Analyse NLP effectu√©e\")\n",
        "\n",
        "    # V√©rifier dans les FAQ\n",
        "    if not faq_df.empty and embedding_model is not None:\n",
        "        faq_question, faq_answer, faq_lang, faq_similarity_score = find_similar_faq(query, threshold=0.6)\n",
        "\n",
        "        if faq_question and faq_similarity_score > 0.6:\n",
        "            print(f\"‚úÖ FAQ similaire trouv√©e (Score: {faq_similarity_score:.3f})\")\n",
        "\n",
        "            # Logger l'interaction\n",
        "            learning_module.log_interaction(query, faq_answer, \"FAQ\", lang, faq_similarity_score)\n",
        "\n",
        "            return faq_answer, f\"FAQ (Score: {faq_similarity_score:.3f})\", nlp_analysis\n",
        "\n",
        "        print(f\"‚ö†Ô∏è Similarit√© FAQ insuffisante ({faq_similarity_score:.3f})\")\n",
        "\n",
        "    # Utiliser l'IA g√©n√©rative\n",
        "    print(\"ü§ñ Utilisation de l'IA g√©n√©rative...\")\n",
        "    answer = call_generative_ai(query)\n",
        "\n",
        "    # Logger l'interaction\n",
        "    learning_module.log_interaction(query, answer, \"IA G√©n√©rative\", lang)\n",
        "\n",
        "    return answer, \"IA G√©n√©rative\", nlp_analysis\n",
        "\n",
        "# =============================================================================\n",
        "# FONCTIONS POUR LES DOCUMENTS\n",
        "# =============================================================================\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    \"\"\"Extraction des PDF\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            for page_num in range(len(reader.pages)):\n",
        "                page_text = reader.pages[page_num].extract_text() or \"\"\n",
        "                page_text = re.sub(r'\\s+', ' ', page_text)\n",
        "                page_text = re.sub(r'\\\\n', ' ', page_text)\n",
        "                text += page_text + \" \"\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur extraction PDF {file_path}: {e}\")\n",
        "        return \"\"\n",
        "    return text.strip()\n",
        "\n",
        "def extract_text_from_docx(file_path):\n",
        "    \"\"\"Extraction des DOCX\"\"\"\n",
        "    text = []\n",
        "    try:\n",
        "        document = Document(file_path)\n",
        "        for paragraph in document.paragraphs:\n",
        "            if paragraph.text.strip():\n",
        "                text.append(paragraph.text)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur extraction DOCX {file_path}: {e}\")\n",
        "        return \"\"\n",
        "    return '\\n'.join(text)\n",
        "\n",
        "def extract_text(file_path):\n",
        "    \"\"\"Dispatch l'extraction selon le type de fichier\"\"\"\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "    if file_extension == '.pdf':\n",
        "        return extract_text_from_pdf(file_path)\n",
        "    elif file_extension == '.docx':\n",
        "        return extract_text_from_docx(file_path)\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Type de fichier non support√©: {file_extension}\")\n",
        "        return \"\"\n",
        "\n",
        "# =============================================================================\n",
        "# INTERFACE GRADIO AM√âLIOR√âE\n",
        "# =============================================================================\n",
        "\n",
        "def gradio_interface(question, uploaded_file, enable_nlp_analysis, show_learning_report):\n",
        "    \"\"\"Interface Gradio avec NLP et apprentissage\"\"\"\n",
        "    response_parts = []\n",
        "\n",
        "    # Afficher le rapport d'apprentissage si demand√©\n",
        "    if show_learning_report:\n",
        "        report = learning_module.analyze_patterns()\n",
        "        response_parts.append(report)\n",
        "        return \"\\n\\n\".join(response_parts)\n",
        "\n",
        "    # Traitement du document\n",
        "    if uploaded_file is not None:\n",
        "        print(f\"üìÑ Traitement du document: {uploaded_file.name}\")\n",
        "        try:\n",
        "            if isinstance(uploaded_file, str):\n",
        "                file_path = uploaded_file\n",
        "                file_name = os.path.basename(file_path)\n",
        "            else:\n",
        "                file_path = uploaded_file.name if hasattr(uploaded_file, 'name') else str(uploaded_file)\n",
        "                file_name = os.path.basename(file_path)\n",
        "\n",
        "            new_text_content = extract_text(file_path)\n",
        "\n",
        "            if new_text_content and len(new_text_content.strip()) > 50:\n",
        "                response_parts.append(f\"**‚úÖ Document '{file_name}' trait√© avec succ√®s.**\")\n",
        "                response_parts.append(f\"**üìä Contenu extrait :** {len(new_text_content)} caract√®res\")\n",
        "\n",
        "                # Analyse NLP du document si activ√©e\n",
        "                if enable_nlp_analysis:\n",
        "                    doc_analysis = nlp_module.full_analysis(new_text_content[:1000])\n",
        "                    response_parts.append(\"\\n**üß† Analyse NLP du document :**\")\n",
        "                    response_parts.append(f\"‚Ä¢ Sentiment: {doc_analysis['sentiment']['label']}\")\n",
        "                    response_parts.append(f\"‚Ä¢ Complexit√©: {doc_analysis['complexity']['level']}\")\n",
        "                    if doc_analysis['keywords']:\n",
        "                        response_parts.append(f\"‚Ä¢ Mots-cl√©s: {', '.join(doc_analysis['keywords'])}\")\n",
        "\n",
        "                print(f\"‚úÖ Document int√©gr√©: {len(new_text_content)} caract√®res\")\n",
        "            else:\n",
        "                response_parts.append(f\"**‚ùå Erreur :** Document vide ou impossible √† extraire.\")\n",
        "\n",
        "            os.unlink(file_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            response_parts.append(f\"**‚ùå Erreur traitement fichier :** {str(e)}\")\n",
        "\n",
        "    # Traitement de la question\n",
        "    if question and question.strip():\n",
        "        print(f\"‚ùì Question re√ßue: {question}\")\n",
        "        answer, source, nlp_analysis = answer_question(question, enable_nlp_analysis)\n",
        "\n",
        "        response_parts.append(f\"**‚ùì Votre question :** {question}\")\n",
        "        response_parts.append(f\"**üí° R√©ponse :** {answer}\")\n",
        "        response_parts.append(f\"**üìö Source :** {source}\")\n",
        "\n",
        "        # Afficher l'analyse NLP si disponible\n",
        "        if nlp_analysis and enable_nlp_analysis:\n",
        "            response_parts.append(\"\\n**üß† Analyse NLP de votre question :**\")\n",
        "            response_parts.append(f\"‚Ä¢ Sentiment: {nlp_analysis['sentiment']['label']} (Score: {nlp_analysis['sentiment']['score']:.3f})\")\n",
        "            response_parts.append(f\"‚Ä¢ Complexit√©: {nlp_analysis['complexity']['level']}\")\n",
        "\n",
        "            if nlp_analysis['entities']:\n",
        "                entities_str = ', '.join([f\"{e['text']} ({e['type']})\" for e in nlp_analysis['entities']])\n",
        "                response_parts.append(f\"‚Ä¢ Entit√©s d√©tect√©es: {entities_str}\")\n",
        "\n",
        "            if nlp_analysis['keywords']:\n",
        "                response_parts.append(f\"‚Ä¢ Mots-cl√©s: {', '.join(nlp_analysis['keywords'])}\")\n",
        "\n",
        "        print(f\"‚úÖ R√©ponse g√©n√©r√©e - Source: {source}\")\n",
        "\n",
        "    if not response_parts:\n",
        "        response_parts.append(\"**‚ÑπÔ∏è Veuillez :**\\n‚Ä¢ Poser une question\\n‚Ä¢ OU uploader un document\\n‚Ä¢ OU voir le rapport d'apprentissage !\")\n",
        "\n",
        "    return \"\\n\\n\".join(response_parts)\n",
        "\n",
        "# Cr√©er l'interface Gradio\n",
        "print(\"üöÄ Cr√©ation de l'interface Gradio am√©lior√©e...\")\n",
        "interface = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=[\n",
        "        gr.Textbox(\n",
        "            label=\"Posez votre question\",\n",
        "            placeholder=\"Ex: Comment m'inscrire √† l'universit√© ?\",\n",
        "            lines=2\n",
        "        ),\n",
        "        gr.File(\n",
        "            label=\"üì§ Uploader un document (PDF/DOCX)\",\n",
        "            file_types=[\".pdf\", \".docx\"]\n",
        "        ),\n",
        "        gr.Checkbox(\n",
        "            label=\"üß† Activer l'analyse NLP avanc√©e\",\n",
        "            value=False\n",
        "        ),\n",
        "        gr.Checkbox(\n",
        "            label=\"üìä Afficher le rapport d'apprentissage\",\n",
        "            value=False\n",
        "        )\n",
        "    ],\n",
        "    outputs=gr.Textbox(\n",
        "        label=\"R√©ponse du syst√®me\",\n",
        "        lines=15,\n",
        "        show_copy_button=True\n",
        "    ),\n",
        "    title=\"üáπüá≥ TuniSpeak - Assistant Trilingue avec IA & Apprentissage\",\n",
        "    description=f\"\"\"\n",
        "    **‚ú® VERSION COMPL√àTE avec NLP et Machine Learning ‚ú®**\n",
        "    ‚úì {len(faq_df)} questions FAQ charg√©es\n",
        "    ‚úì Analyse NLP avanc√©e (sentiment, entit√©s, mots-cl√©s)\n",
        "    ‚úì Module d'apprentissage continu\n",
        "    ‚úì {len(learning_module.interactions_log)} interactions enregistr√©es\n",
        "    ‚úì Support trilingue (fran√ßais, arabe, darija)\n",
        "    \"\"\",\n",
        "    examples=[\n",
        "        [\"Comment m'inscrire √† l'universit√© ?\", None, True, False],\n",
        "        [\"ŸÉŸäŸÅ ÿ£ÿ≥ÿ¨ŸÑ ŸÅŸä ÿßŸÑÿ¨ÿßŸÖÿπÿ©ÿü\", None, False, False],\n",
        "        [\"Quels sont les frais de scolarit√© ?\", None, True, False],\n",
        "        [\"\", None, False, True]  # Voir le rapport\n",
        "    ]\n",
        ")\n",
        "\n",
        "# =============================================================================\n",
        "# APPLICATION FASTAPI AM√âLIOR√âE\n",
        "# =============================================================================\n",
        "\n",
        "app = FastAPI(title=\"TuniSpeak API\", version=\"3.0\")\n",
        "\n",
        "os.makedirs(\"/content/templates\", exist_ok=True)\n",
        "\n",
        "html_content = '''\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"fr\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>üáπüá≥ TuniSpeak - Assistant Trilingue IA</title>\n",
        "    <style>\n",
        "        * { box-sizing: border-box; margin: 0; padding: 0; }\n",
        "        body {\n",
        "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
        "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "            min-height: 100vh;\n",
        "            padding: 20px;\n",
        "        }\n",
        "        .container {\n",
        "            max-width: 1000px;\n",
        "            margin: 0 auto;\n",
        "            background: white;\n",
        "            padding: 30px;\n",
        "            border-radius: 15px;\n",
        "            box-shadow: 0 10px 30px rgba(0,0,0,0.2);\n",
        "        }\n",
        "        h1 {\n",
        "            color: #2c3e50;\n",
        "            text-align: center;\n",
        "            margin-bottom: 10px;\n",
        "        }\n",
        "        .subtitle {\n",
        "            text-align: center;\n",
        "            color: #7f8c8d;\n",
        "            margin-bottom: 20px;\n",
        "        }\n",
        "        .badge {\n",
        "            display: inline-block;\n",
        "            background: #3498db;\n",
        "            color: white;\n",
        "            padding: 5px 12px;\n",
        "            border-radius: 15px;\n",
        "            font-size: 12px;\n",
        "            margin: 0 5px;\n",
        "        }\n",
        "        .stats {\n",
        "            display: grid;\n",
        "            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n",
        "            gap: 15px;\n",
        "            margin: 20px 0;\n",
        "        }\n",
        "        .stat-card {\n",
        "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "            color: white;\n",
        "            padding: 20px;\n",
        "            border-radius: 10px;\n",
        "            text-align: center;\n",
        "        }\n",
        "        .stat-number {\n",
        "            font-size: 32px;\n",
        "            font-weight: bold;\n",
        "            margin: 10px 0;\n",
        "        }\n",
        "        .stat-label {\n",
        "            font-size: 14px;\n",
        "            opacity: 0.9;\n",
        "        }\n",
        "        .input-group {\n",
        "            margin-bottom: 20px;\n",
        "        }\n",
        "        label {\n",
        "            display: block;\n",
        "            margin-bottom: 8px;\n",
        "            font-weight: 600;\n",
        "            color: #2c3e50;\n",
        "        }\n",
        "        textarea, input[type=\"file\"] {\n",
        "            width: 100%;\n",
        "            padding: 12px;\n",
        "            border: 2px solid #ddd;\n",
        "            border-radius: 8px;\n",
        "            font-size: 16px;\n",
        "            transition: border-color 0.3s;\n",
        "        }\n",
        "        textarea:focus, input[type=\"file\"]:focus {\n",
        "            border-color: #3498db;\n",
        "            outline: none;\n",
        "        }\n",
        "        textarea {\n",
        "            height: 120px;\n",
        "            resize: vertical;\n",
        "        }\n",
        "        .checkbox-group {\n",
        "            margin: 15px 0;\n",
        "        }\n",
        "        .checkbox-label {\n",
        "            display: flex;\n",
        "            align-items: center;\n",
        "            margin: 10px 0;\n",
        "            cursor: pointer;\n",
        "        }\n",
        "        .checkbox-label input {\n",
        "            margin-right: 10px;\n",
        "            width: 20px;\n",
        "            height: 20px;\n",
        "            cursor: pointer;\n",
        "        }\n",
        "        button {\n",
        "            background: #3498db;\n",
        "            color: white;\n",
        "            border: none;\n",
        "            padding: 15px 30px;\n",
        "            font-size: 18px;\n",
        "            border-radius: 8px;\n",
        "            cursor: pointer;\n",
        "            width: 100%;\n",
        "            transition: background 0.3s;\n",
        "            margin-top: 10px;\n",
        "        }\n",
        "        button:hover {\n",
        "            background: #2980b9;\n",
        "        }\n",
        "        button.secondary {\n",
        "            background: #95a5a6;\n",
        "        }\n",
        "        button.secondary:hover {\n",
        "            background: #7f8c8d;\n",
        "        }\n",
        "        #response {\n",
        "            margin-top: 30px;\n",
        "            padding: 20px;\n",
        "            background: #f8f9fa;\n",
        "            border-radius: 8px;\n",
        "            border-left: 4px solid #3498db;\n",
        "            white-space: pre-wrap;\n",
        "            max-height: 600px;\n",
        "            overflow-y: auto;\n",
        "        }\n",
        "        #loading {\n",
        "            display: none;\n",
        "            text-align: center;\n",
        "            padding: 20px;\n",
        "            color: #3498db;\n",
        "        }\n",
        "        .features {\n",
        "            display: grid;\n",
        "            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n",
        "            gap: 15px;\n",
        "            margin: 20px 0;\n",
        "        }\n",
        "        .feature {\n",
        "            text-align: center;\n",
        "            padding: 15px;\n",
        "            background: #f8f9fa;\n",
        "            border-radius: 8px;\n",
        "            border: 1px solid #e9ecef;\n",
        "        }\n",
        "        .feature-icon {\n",
        "            font-size: 24px;\n",
        "            margin-bottom: 10px;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"container\">\n",
        "        <h1>üáπüá≥ TuniSpeak Assistant IA</h1>\n",
        "        <div class=\"subtitle\">\n",
        "            <span class=\"badge\">üß† NLP</span>\n",
        "            <span class=\"badge\">üìä ML</span>\n",
        "            <span class=\"badge\">üåç Trilingue</span>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"stats\">\n",
        "            <div class=\"stat-card\">\n",
        "                <div class=\"stat-label\">Questions FAQ</div>\n",
        "                <div class=\"stat-number\">{{ faq_count }}</div>\n",
        "            </div>\n",
        "            <div class=\"stat-card\">\n",
        "                <div class=\"stat-label\">Interactions</div>\n",
        "                <div class=\"stat-number\">{{ interactions_count }}</div>\n",
        "            </div>\n",
        "            <div class=\"stat-card\">\n",
        "                <div class=\"stat-label\">Feedbacks</div>\n",
        "                <div class=\"stat-number\">{{ feedback_count }}</div>\n",
        "            </div>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"features\">\n",
        "            <div class=\"feature\">\n",
        "                <div class=\"feature-icon\">üß†</div>\n",
        "                <div>Analyse NLP</div>\n",
        "            </div>\n",
        "            <div class=\"feature\">\n",
        "                <div class=\"feature-icon\">üìö</div>\n",
        "                <div>Base FAQ</div>\n",
        "            </div>\n",
        "            <div class=\"feature\">\n",
        "                <div class=\"feature-icon\">üìÑ</div>\n",
        "                <div>Documents</div>\n",
        "            </div>\n",
        "            <div class=\"feature\">\n",
        "                <div class=\"feature-icon\">üìä</div>\n",
        "                <div>Apprentissage</div>\n",
        "            </div>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"input-group\">\n",
        "            <label for=\"question\">üí¨ Votre question :</label>\n",
        "            <textarea id=\"question\" placeholder=\"Posez votre question en fran√ßais, arabe ou darija...\"></textarea>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"input-group\">\n",
        "            <label for=\"fileUpload\">üì§ Document (PDF/DOCX) :</label>\n",
        "            <input type=\"file\" id=\"fileUpload\" accept=\".pdf,.docx\">\n",
        "        </div>\n",
        "\n",
        "        <div class=\"checkbox-group\">\n",
        "            <label class=\"checkbox-label\">\n",
        "                <input type=\"checkbox\" id=\"enableNLP\">\n",
        "                üß† Activer l'analyse NLP avanc√©e\n",
        "            </label>\n",
        "        </div>\n",
        "\n",
        "        <button onclick=\"submitQuestion()\">üöÄ Envoyer</button>\n",
        "        <button class=\"secondary\" onclick=\"showLearningReport()\">üìä Rapport d'Apprentissage</button>\n",
        "\n",
        "        <div id=\"loading\">\n",
        "            <div>‚è≥ Traitement en cours...</div>\n",
        "        </div>\n",
        "\n",
        "        <div id=\"response\"></div>\n",
        "    </div>\n",
        "\n",
        "    <script>\n",
        "        async function submitQuestion() {\n",
        "            const question = document.getElementById(\"question\").value;\n",
        "            const fileInput = document.getElementById(\"fileUpload\");\n",
        "            const file = fileInput.files[0];\n",
        "            const enableNLP = document.getElementById(\"enableNLP\").checked;\n",
        "            const responseDiv = document.getElementById(\"response\");\n",
        "            const loadingDiv = document.getElementById(\"loading\");\n",
        "\n",
        "            if (!question.trim() && !file) {\n",
        "                responseDiv.innerHTML = \"<strong>‚ùå Attention :</strong> Veuillez poser une question ou uploader un document.\";\n",
        "                return;\n",
        "            }\n",
        "\n",
        "            loadingDiv.style.display = \"block\";\n",
        "            responseDiv.innerHTML = \"\";\n",
        "\n",
        "            const formData = new FormData();\n",
        "            if (question.trim()) formData.append(\"question\", question);\n",
        "            if (file) formData.append(\"uploaded_file\", file);\n",
        "            formData.append(\"enable_nlp\", enableNLP);\n",
        "\n",
        "            try {\n",
        "                const response = await fetch(\"/qa\", {\n",
        "                    method: \"POST\",\n",
        "                    body: formData\n",
        "                });\n",
        "\n",
        "                if (!response.ok) throw new Error(`HTTP ${response.status}`);\n",
        "\n",
        "                const data = await response.json();\n",
        "                let htmlResponse = data.answer.replace(/\\\\n/g, '<br>');\n",
        "                responseDiv.innerHTML = htmlResponse;\n",
        "\n",
        "            } catch (error) {\n",
        "                console.error(\"Error:\", error);\n",
        "                responseDiv.innerHTML = `<strong>‚ùå Erreur :</strong> ${error.message}`;\n",
        "            } finally {\n",
        "                loadingDiv.style.display = \"none\";\n",
        "            }\n",
        "        }\n",
        "\n",
        "        async function showLearningReport() {\n",
        "            const responseDiv = document.getElementById(\"response\");\n",
        "            const loadingDiv = document.getElementById(\"loading\");\n",
        "\n",
        "            loadingDiv.style.display = \"block\";\n",
        "            responseDiv.innerHTML = \"\";\n",
        "\n",
        "            try {\n",
        "                const response = await fetch(\"/learning-report\");\n",
        "                if (!response.ok) throw new Error(`HTTP ${response.status}`);\n",
        "\n",
        "                const data = await response.json();\n",
        "                responseDiv.innerHTML = data.report.replace(/\\\\n/g, '<br>');\n",
        "\n",
        "            } catch (error) {\n",
        "                console.error(\"Error:\", error);\n",
        "                responseDiv.innerHTML = `<strong>‚ùå Erreur :</strong> ${error.message}`;\n",
        "            } finally {\n",
        "                loadingDiv.style.display = \"none\";\n",
        "            }\n",
        "        }\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "'''\n",
        "\n",
        "with open(\"/content/templates/index.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(html_content)\n",
        "\n",
        "templates = Jinja2Templates(directory=\"/content/templates\")\n",
        "\n",
        "@app.post(\"/qa\")\n",
        "async def qa_endpoint(\n",
        "    question: str = Form(\"\"),\n",
        "    uploaded_file: UploadFile = File(None),\n",
        "    enable_nlp: bool = Form(False)\n",
        "):\n",
        "    \"\"\"Endpoint avec support NLP\"\"\"\n",
        "    response_text = \"\"\n",
        "\n",
        "    if uploaded_file and uploaded_file.filename:\n",
        "        try:\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.filename)[1]) as tmp_file:\n",
        "                content = await uploaded_file.read()\n",
        "                tmp_file.write(content)\n",
        "                tmp_path = tmp_file.name\n",
        "\n",
        "            new_text_content = extract_text(tmp_path)\n",
        "\n",
        "            if new_text_content and len(new_text_content.strip()) > 50:\n",
        "                response_text += f\"**‚úÖ Document '{uploaded_file.filename}' trait√©.**\\n\\n\"\n",
        "\n",
        "                if enable_nlp:\n",
        "                    doc_analysis = nlp_module.full_analysis(new_text_content[:1000])\n",
        "                    response_text += f\"**üß† Analyse NLP:** Sentiment: {doc_analysis['sentiment']['label']}, Complexit√©: {doc_analysis['complexity']['level']}\\n\\n\"\n",
        "\n",
        "            os.unlink(tmp_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            response_text += f\"**‚ùå Erreur fichier:** {str(e)}\\n\\n\"\n",
        "\n",
        "    if question and question.strip():\n",
        "        try:\n",
        "            answer, source, nlp_analysis = answer_question(question, enable_nlp)\n",
        "            response_text += f\"**‚ùì Question:** {question}\\n\\n\"\n",
        "            response_text += f\"**üí° R√©ponse:** {answer}\\n\\n\"\n",
        "            response_text += f\"**üìö Source:** {source}\\n\\n\"\n",
        "\n",
        "            if nlp_analysis and enable_nlp:\n",
        "                response_text += f\"**üß† Analyse NLP:** Sentiment: {nlp_analysis['sentiment']['label']}, Complexit√©: {nlp_analysis['complexity']['level']}\\n\"\n",
        "\n",
        "        except Exception as e:\n",
        "            response_text += f\"**‚ùå Erreur:** {str(e)}\\n\\n\"\n",
        "\n",
        "    if not response_text.strip():\n",
        "        response_text = \"**‚ÑπÔ∏è Veuillez poser une question ou uploader un document.**\"\n",
        "\n",
        "    return {\"answer\": response_text}\n",
        "\n",
        "@app.get(\"/learning-report\")\n",
        "async def learning_report():\n",
        "    \"\"\"Endpoint pour le rapport d'apprentissage\"\"\"\n",
        "    report = learning_module.analyze_patterns()\n",
        "    return {\"report\": report}\n",
        "\n",
        "@app.get(\"/\", response_class=HTMLResponse)\n",
        "async def get_index(request: Request):\n",
        "    return templates.TemplateResponse(\"index.html\", {\n",
        "        \"request\": request,\n",
        "        \"faq_count\": len(faq_df),\n",
        "        \"interactions_count\": len(learning_module.interactions_log),\n",
        "        \"feedback_count\": len(learning_module.feedback_log)\n",
        "    })\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    return {\n",
        "        \"status\": \"healthy\",\n",
        "        \"service\": \"TuniSpeak API v3.0\",\n",
        "        \"features\": [\"NLP\", \"ML\", \"FAQ\"],\n",
        "        \"faq_count\": len(faq_df),\n",
        "        \"interactions\": len(learning_module.interactions_log)\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# LANCEMENT\n",
        "# =============================================================================\n",
        "\n",
        "def launch_gradio():\n",
        "    print(\"üöÄ Lancement de Gradio...\")\n",
        "    return interface.launch(share=True, quiet=True, inbrowser=False)\n",
        "\n",
        "def launch_fastapi():\n",
        "    try:\n",
        "        os.system(\"fuser -k 8000/tcp 2>/dev/null || true\")\n",
        "        time.sleep(2)\n",
        "\n",
        "        print(\"üöÄ Lancement FastAPI...\")\n",
        "        config = uvicorn.Config(app=app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "        server = uvicorn.Server(config)\n",
        "\n",
        "        try:\n",
        "            ngrok_token = userdata.get(\"NGROK_AUTH_TOKEN\")\n",
        "            if ngrok_token:\n",
        "                ngrok.set_auth_token(ngrok_token)\n",
        "                public_url = ngrok.connect(8000).public_url\n",
        "                print(f\"üåç Application: {public_url}\")\n",
        "        except:\n",
        "            print(\"üåê Local: http://localhost:8000\")\n",
        "\n",
        "        loop = asyncio.get_event_loop()\n",
        "        loop.run_until_complete(server.serve())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur: {e}\")\n",
        "\n",
        "# Tests\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üß™ TESTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_questions = [\n",
        "    \"Comment m'inscrire √† l'universit√© ?\",\n",
        "    \"ŸÉŸäŸÅ ÿ£ÿ≥ÿ¨ŸÑ ŸÅŸä ÿßŸÑÿ¨ÿßŸÖÿπÿ©ÿü\",\n",
        "    \"Quels sont les frais de scolarit√© ?\"\n",
        "]\n",
        "\n",
        "for question in test_questions:\n",
        "    print(f\"\\n--- Test: {question} ---\")\n",
        "    answer, source, nlp_analysis = answer_question(question, True)\n",
        "    print(f\"R√©ponse: {answer[:100]}...\")\n",
        "    print(f\"Source: {source}\")\n",
        "    if nlp_analysis:\n",
        "        print(f\"Sentiment: {nlp_analysis['sentiment']['label']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ SYST√àME PR√äT avec NLP et ML !\")\n",
        "print(\"=\"*60)\n",
        "print(f\"üìä FAQ: {len(faq_df)} | Interactions: {len(learning_module.interactions_log)}\")\n",
        "\n",
        "gradio_thread = threading.Thread(target=launch_gradio, daemon=True)\n",
        "gradio_thread.start()\n",
        "\n",
        "import time\n",
        "time.sleep(3)\n",
        "launch_fastapi()"
      ]
    }
  ]
}